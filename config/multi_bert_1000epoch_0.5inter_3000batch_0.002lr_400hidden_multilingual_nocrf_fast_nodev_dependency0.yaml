ModelFinetuner:
  direct_upsample_rate: -1
  distill_mode: false
  down_sample_amount: -1
  ensemble_distill_mode: false
  language_resample: false
  optimizer: Adam
  train_with_professor: false
dependency:
  Corpus: UD_English-EWT:UD_Hebrew-HTB:UD_Japanese-GSD:UD_Slovenian-SST:UD_French-Sequoia:UD_Indonesian-GSD:UD_Persian-Seraji:UD_Tamil-TTB:UD_Dutch-LassySmall:UD_German-GSD:UD_Swedish-LinES:UD_Italian-PoSTWITA:UD_Spanish-GSD
  tag_dictionary: resources/taggers/ud_dependency_tags.pkl
embeddings:
  BertEmbeddings:
    bert_model_or_path: bert-base-multilingual-cased
    layers: '-1'
    pooling_operation: mean
    sentence_feat: false
is_teacher_list: true
is_toy: false
model:
  SemanticDependencyParser:
    binary: false
    factorize: true
    hidden_size: 400
    init_std: 0.25
    interpolation: 0.5
    iterations: 3
    lstm_dropout: 0.33
    mlp_dropout: 0.33
    n_mlp_arc: 500
    n_mlp_rel: 100
    n_mlp_sec: 150
    rnn_layers: 3
    tree: true
    use_cop: true
    use_crf: false
    use_gp: true
    use_rnn: true
    use_second_order: false
    use_sib: true
    word_dropout: 0.33
model_name: multi_bert_1000epoch_0.5inter_3000batch_0.002lr_400hidden_multilingual_nocrf_fast_nodev_dependency0
target_dir: resources/taggers/
targets: dependency
teacher_annealing: false
train:
  best_k: 5
  betas:
  - 0.9
  - 0.9
  calc_teachers_target_loss: false
  entropy_loss_rate: 0.001
  fine_tune_mode: false
  language_attention_entropy: false
  language_attention_warmup: false
  language_attention_warmup_and_fix: false
  learning_rate: 0.002
  lr_rate: 1
  max_epochs: 1000
  min_freq: 2
  mini_batch_size: 3000
  monitor_test: false
  save_final_model: false
  sort_data: true
  train_language_attention_by_dev: false
  train_with_dev: false
  true_reshuffle: false
  use_unlabeled_data: false
trainer: ModelFinetuner
