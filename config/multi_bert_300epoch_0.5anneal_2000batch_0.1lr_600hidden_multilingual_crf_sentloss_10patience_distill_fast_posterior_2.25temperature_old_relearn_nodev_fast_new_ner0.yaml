ModelDistiller:
  distill_mode: true
  optimizer: SGD
  train_with_professor: false
anneal_factor: 0.5
embeddings:
  BertEmbeddings:
    bert_model_or_path: bert-base-multilingual-cased
    layers: '-1'
    pooling_operation: mean
interpolation: 0.5
is_teacher_list: true
model:
  FastSequenceTagger:
    crf_attention: false
    distill_crf: false
    distill_posterior: true
    dropout: 0.0
    hidden_size: 600
    relearn_embeddings: true
    sentence_loss: true
    temperature: 2.25
    use_crf: true
model_name: multi_bert_300epoch_0.5anneal_2000batch_0.1lr_600hidden_multilingual_crf_sentloss_10patience_distill_fast_posterior_2.25temperature_old_relearn_nodev_fast_new_ner0
ner:
  Corpus: CONLL_03_DUTCH:CONLL_03_SPANISH:CONLL_03:CONLL_03_GERMAN
  tag_dictionary: resources/taggers/ner_tags.pkl
  teachers:
    ? config/multi_bert_origflair_300epoch_2000batch_1lr_256hidden_de_monolingual_crf_sentloss_10patience_baseline_fast_nodev_ner12.yaml
    : CONLL_03_GERMAN
    ? config/multi_bert_origflair_300epoch_2000batch_1lr_256hidden_en_monolingual_crf_sentloss_10patience_baseline_fast_nodev_ner11.yaml
    : CONLL_03
    ? config/multi_bert_origflair_300epoch_2000batch_1lr_256hidden_es_monolingual_crf_sentloss_10patience_baseline_fast_nodev_ner12.yaml
    : CONLL_03_SPANISH
    ? config/multi_bert_origflair_300epoch_2000batch_1lr_256hidden_nl_monolingual_crf_sentloss_10patience_baseline_fast_nodev_ner11.yaml
    : CONLL_03_DUTCH
target_dir: resources/taggers/
targets: ner
teacher_annealing: true
train:
  learning_rate: 0.1
  max_epochs: 300
  mini_batch_size: 2000
  monitor_test: false
  patience: 10
  professor_interpolation: 0.5
  save_final_model: false
  train_with_dev: false
  true_reshuffle: false
trainer: ModelDistiller
